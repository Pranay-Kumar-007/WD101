from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_community.llms import Ollama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.retrievers import BM25Retriever
# Using built-in alternatives instead of sklearn for offline compatibility
import os
import pickle
import hashlib
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import warnings
import re
from typing import List, Tuple
warnings.filterwarnings("ignore")

class AdvancedRAGSystem:
    def __init__(self):
        print("üîß Initializing Advanced RAG System with hybrid search...")
        
        # Initialize embedding model with multiple fallbacks
        self.embed = None
        embedding_models = ["mxbai-embed", "nomic-embed-text", "llama2"]
        
        for model_name in embedding_models:
            try:
                print(f"üîß Trying embedding model: {model_name}...")
                self.embed = OllamaEmbeddings(
                    model=model_name,
                    show_progress=False,
                    base_url="http://localhost:11434",
                    temperature=0.1 if model_name == "mxbai-embed" else 0.0,
                    num_ctx=512 if model_name == "mxbai-embed" else 1024
                )
                # Test the embedding with a simple check
                print(f"‚úÖ Successfully initialized embedding model: {model_name}")
                break
            except Exception as e:
                print(f"‚ùå Failed to initialize {model_name}: {e}")
                self.embed = None
                continue
        
        if self.embed is None:
            raise Exception("Failed to initialize any embedding model. Please check Ollama installation.")
        
        # Initialize query rewriter LLM
        self.query_rewriter = Ollama(
            model="gemma3:1b",
            temperature=0.3,
            num_predict=100,
            num_ctx=1024
        )
        
        # Initialize reranker LLM
        self.reranker = Ollama(
            model="gemma3:1b",
            temperature=0.1,
            num_predict=50,
            num_ctx=2048
        )
        
        self.vectorstore = None
        self.bm25_retriever = None
        self.documents = []
        self.tfidf_vectorizer = None
        self.document_embeddings = None
        
        print("‚úÖ Advanced RAG components initialized")
    
    def rewrite_query(self, original_query: str) -> List[str]:
        """Generate multiple query variations for better retrieval"""
        rewrite_prompt = f"""Generate 3 different ways to ask the same question. Make them more specific and detailed.
        
Original question: {original_query}
        
Provide 3 alternative questions (one per line):
        1.
        2.
        3."""
        
        try:
            response = self.query_rewriter.invoke(rewrite_prompt)
            queries = [original_query]  # Always include original
            
            # Parse the response to extract queries
            lines = response.strip().split('\n')
            for line in lines:
                line = line.strip()
                if line and (line.startswith('1.') or line.startswith('2.') or line.startswith('3.')):
                    query = line[2:].strip()
                    if query and query not in queries:
                        queries.append(query)
            
            return queries[:4]  # Return up to 4 queries total
        except:
            return [original_query]
    
    def hybrid_search(self, query: str, k: int = 6) -> List[Document]:
        """Combine vector search and BM25 for better retrieval"""
        all_results = []
        
        # Get multiple query variations
        query_variations = self.rewrite_query(query)
        
        for q in query_variations:
            # Ensure k is integer and calculate half values
            k_int = int(k) if isinstance(k, float) else k
            half_k = max(1, k_int // 2)  # Ensure at least 1
            
            # Vector search with null check
            try:
                if self.vectorstore is not None:
                    vector_results = self.vectorstore.similarity_search(q, k=half_k)
                    for doc in vector_results:
                        doc.metadata['search_type'] = 'vector'
                        doc.metadata['query_variant'] = q
                    all_results.extend(vector_results)
                else:
                    print(f"‚ö†Ô∏è Vectorstore is None, skipping vector search for query '{q}'")
            except Exception as e:
                print(f"Vector search error for query '{q}': {e}")
                pass
            
            # BM25 search
            if self.bm25_retriever:
                try:
                    bm25_results = self.bm25_retriever.get_relevant_documents(q)
                    for doc in bm25_results[:half_k]:
                        doc.metadata['search_type'] = 'bm25'
                        doc.metadata['query_variant'] = q
                    all_results.extend(bm25_results[:half_k])
                except Exception as e:
                    print(f"BM25 search error for query '{q}': {e}")
                    pass
        
        # Remove duplicates based on content
        unique_results = []
        seen_content = set()
        
        for doc in all_results:
            content_hash = hashlib.md5(doc.page_content[:200].encode()).hexdigest()
            if content_hash not in seen_content:
                unique_results.append(doc)
                seen_content.add(content_hash)
        
        # Fallback if no results found
        if not unique_results:
            print(f"‚ö†Ô∏è No search results found for query: '{query}'")
            if self.documents:
                print(f"   Returning first {min(int(k), len(self.documents))} documents as fallback")
                k_int = int(k) if isinstance(k, float) else k
                fallback_docs = self.documents[:k_int]
                for doc in fallback_docs:
                    doc.metadata['search_type'] = 'fallback'
                    doc.metadata['query_variant'] = query
                return fallback_docs
            else:
                print("   No documents available for fallback")
                return []
        
        k_int = int(k) if isinstance(k, float) else k
        return unique_results[:k_int*2]  # Return more for reranking
    
    def rerank_documents(self, query: str, documents: List[Document]) -> List[Document]:
        """Rerank documents using LLM-based relevance scoring"""
        if len(documents) <= 3:
            return documents
        
        scored_docs = []
        
        for doc in documents:
            # Create relevance scoring prompt
            score_prompt = f"""Rate how relevant this document is to the question on a scale of 1-10.
            
Question: {query}
            
Document: {doc.page_content[:500]}...
            
Provide only a number from 1-10 (10 = highly relevant, 1 = not relevant):"""
            
            try:
                response = self.reranker.invoke(score_prompt)
                # Extract score from response
                score_match = re.search(r'(\d+)', response.strip())
                if score_match:
                    score = int(float(score_match.group(1)))  # Handle float strings
                else:
                    score = 5
                score = max(1, min(10, score))  # Clamp to 1-10
            except Exception as e:
                print(f"Scoring error: {e}")
                score = 5  # Default score
            
            scored_docs.append((score, doc))
        
        # Sort by score descending
        scored_docs.sort(key=lambda x: x[0], reverse=True)
        
        # Return top documents
        return [doc for score, doc in scored_docs[:6]]
    
    def structured_query_analysis(self, query: str) -> dict:
        """Analyze query structure to optimize retrieval strategy"""
        analysis_prompt = f"""Analyze this question and identify:
        1. Question type (factual, analytical, comparison, etc.)
        2. Key entities/topics
        3. Suggested search strategy
        
Question: {query}
        
Provide analysis in this format:
Type: [question type]
Entities: [key entities]
Strategy: [search strategy]"""
        
        try:
            response = self.query_rewriter.invoke(analysis_prompt)
            
            analysis = {
                'type': 'general',
                'entities': [],
                'strategy': 'standard'
            }
            
            lines = response.strip().split('\n')
            for line in lines:
                if line.startswith('Type:'):
                    analysis['type'] = line.split(':', 1)[1].strip().lower()
                elif line.startswith('Entities:'):
                    entities = line.split(':', 1)[1].strip()
                    analysis['entities'] = [e.strip() for e in entities.split(',') if e.strip()]
                elif line.startswith('Strategy:'):
                    analysis['strategy'] = line.split(':', 1)[1].strip().lower()
            
            return analysis
        except:
            return {'type': 'general', 'entities': [], 'strategy': 'standard'}

# Initialize the advanced RAG system
advanced_rag = AdvancedRAGSystem()

# Cache file paths
cache_dir = "D:/GenAI/cache"
os.makedirs(cache_dir, exist_ok=True)
vectorstore_cache = os.path.join(cache_dir, "vectorstore.pkl")
docs_hash_cache = os.path.join(cache_dir, "docs_hash.txt")

# Get document hash for cache validation
docs_dir = "D:/GenAI/docs"
docs_hash = ""
if os.path.exists(docs_dir):
    # Support multiple document formats for caching
    supported_extensions = ['.pdf', '.docx', '.doc']
    document_files = [f for f in os.listdir(docs_dir) 
                     if any(f.lower().endswith(ext) for ext in supported_extensions)]
    if document_files:
        # Create hash based on file names and modification times
        hash_input = "".join([f"{f}_{os.path.getmtime(os.path.join(docs_dir, f))}" for f in sorted(document_files)])
        docs_hash = hashlib.md5(hash_input.encode()).hexdigest()

# Check if we can use cached vectorstore
use_cache = False
if os.path.exists(vectorstore_cache) and os.path.exists(docs_hash_cache):
    try:
        with open(docs_hash_cache, 'r') as f:
            cached_hash = f.read().strip()
        if cached_hash == docs_hash:
            use_cache = True
            print("Using cached vectorstore for faster startup...")
    except:
        pass

if use_cache:
    try:
        with open(vectorstore_cache, 'rb') as f:
            cached_vectorstore = pickle.load(f)
        # Properly assign to advanced_rag.vectorstore
        advanced_rag.vectorstore = cached_vectorstore
        print("Cached vectorstore loaded successfully.")
    except Exception as e:
        use_cache = False
        print(f"Cache corrupted ({e}), rebuilding...")
        advanced_rag.vectorstore = None

if not use_cache:
    start_time = time.time()
    
    def load_file(file_path):
        """Load document file (PDF or Word) and return documents"""
        try:
            file_extension = os.path.splitext(file_path)[1].lower()
            
            if file_extension == '.pdf':
                loader = PyPDFLoader(file_path)
            elif file_extension in ['.docx', '.doc']:
                loader = Docx2txtLoader(file_path)
            else:
                return [], f"Unsupported file type: {file_extension}"
            
            docs = loader.load()
            # Add file type to metadata
            for doc in docs:
                if hasattr(doc, 'metadata'):
                    doc.metadata['file_type'] = file_extension
                    doc.metadata['loader_type'] = 'pdf' if file_extension == '.pdf' else 'word'
            
            return docs, None
        except Exception as e:
            return [], str(e)
    
    if os.path.exists(docs_dir):
        # Support multiple document formats
        supported_extensions = ['.pdf', '.docx', '.doc']
        document_files = [f for f in os.listdir(docs_dir) 
                         if any(f.lower().endswith(ext) for ext in supported_extensions)]
        
        if document_files:
            print(f"‚ö° Processing {len(document_files)} document files with advanced techniques...")
            
            # Group files by type for reporting
            pdf_files = [f for f in document_files if f.lower().endswith('.pdf')]
            word_files = [f for f in document_files if f.lower().endswith(('.docx', '.doc'))]
            
            print(f"   üìÑ PDF files: {len(pdf_files)}")
            print(f"   üìù Word files: {len(word_files)}")
            
            with ThreadPoolExecutor(max_workers=min(4, len(document_files))) as executor:
                future_to_file = {
                    executor.submit(load_file, os.path.join(docs_dir, file)): file 
                    for file in document_files
                }
                
                for i, future in enumerate(as_completed(future_to_file)):
                    file = future_to_file[future]
                    file_type = "üìÑ" if file.lower().endswith('.pdf') else "üìù"
                    print(f"{file_type} {file} ({i+1}/{len(document_files)})...", end=" ")
                    
                    docs, error = future.result()
                    if error:
                        print(f"‚ùå {error}")
                    else:
                        advanced_rag.documents.extend(docs)
                        pages_or_sections = "pages" if file.lower().endswith('.pdf') else "sections"
                        print(f"‚úÖ {len(docs)} {pages_or_sections}")
        else:
            print("üìÅ No supported document files found in docs directory")
        
        load_time = time.time() - start_time
        print(f"üìö Loaded {len(advanced_rag.documents)} total document sections in {load_time:.2f}s")
    
    if not advanced_rag.documents:
        # Create dummy document if no PDFs found
        advanced_rag.documents = [Document(
            page_content="This is a comprehensive sample document for testing purposes. It contains various topics and information that can be used to test the retrieval system effectively with advanced RAG techniques including hybrid search, query rewriting, and document reranking. This document simulates content that could come from PDF or Word documents.", 
            metadata={"source": "dummy", "type": "test", "file_type": ".txt", "loader_type": "dummy"}
        )]
        print("üìù No supported document files found, using enhanced dummy document")
    
    # Advanced text splitter with semantic awareness
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,   # Smaller chunks for better precision
        chunk_overlap=150, # Overlap for context continuity
        add_start_index=True,
        separators=["\n\n", "\n", ".", "!", "?", ";", ":", ",", " ", ""],
        keep_separator=True,
        is_separator_regex=False,
        length_function=len
    )
    
    split_start = time.time()
    print(f"üî™ Splitting {len(advanced_rag.documents)} documents with advanced chunking...")
    all_split_docs = text_splitter.split_documents(advanced_rag.documents)
    split_time = time.time() - split_start
    print(f"‚úÇÔ∏è Created {len(all_split_docs)} optimized chunks in {split_time:.2f}s")
    
    # Build advanced retrieval system
    index_start = time.time()
    print(f"üèóÔ∏è Building advanced hybrid search system...")
    
    # Create FAISS vector store
    batch_size = 50  # Reasonable batch size for efficiency
    
    print(f"üèóÔ∏è Creating FAISS vector store from {len(all_split_docs)} document chunks...")
    
    try:
        if len(all_split_docs) > batch_size:
            print(f"üì¶ Building vector index in batches of {batch_size}...")
            advanced_rag.vectorstore = None
            
            for i in range(0, len(all_split_docs), batch_size):
                batch_docs = all_split_docs[i:i+batch_size]
                batch_num = i//batch_size + 1
                total_batches = (len(all_split_docs)-1)//batch_size + 1
                print(f"‚ö° Vector batch {batch_num}/{total_batches} ({len(batch_docs)} docs)...", end=" ")
                
                try:
                    if advanced_rag.vectorstore is None:
                        advanced_rag.vectorstore = FAISS.from_documents(batch_docs, advanced_rag.embed)
                        print("‚úÖ Initial batch created")
                    else:
                        batch_vectorstore = FAISS.from_documents(batch_docs, advanced_rag.embed)
                        advanced_rag.vectorstore.merge_from(batch_vectorstore)
                        print("‚úÖ Merged")
                except Exception as e:
                    print(f"‚ùå Batch error: {e}")
                    # Don't continue on batch errors - this could leave vectorstore in bad state
                    if advanced_rag.vectorstore is None:
                        raise e
        else:
            print(f"üìÑ Creating single vector index for {len(all_split_docs)} documents...")
            advanced_rag.vectorstore = FAISS.from_documents(all_split_docs, advanced_rag.embed)
            print("‚úÖ Vector index created successfully")
            
    except Exception as e:
        print(f"‚ùå Vector store creation failed: {e}")
        advanced_rag.vectorstore = None
    
    # Create BM25 retriever for keyword search
    print("üîç Building BM25 keyword index...")
    try:
        advanced_rag.bm25_retriever = BM25Retriever.from_documents(all_split_docs)
        advanced_rag.bm25_retriever.k = int(6)  # Ensure integer
        print("‚úÖ BM25 index created")
    except Exception as e:
        print(f"‚ö†Ô∏è  BM25 creation failed: {e}")
        print("   Continuing with vector-only search...")
        advanced_rag.bm25_retriever = None
    
    # Ensure vectorstore is properly set
    if advanced_rag.vectorstore is None:
        print("‚ö†Ô∏è Warning: Vectorstore creation failed, creating fallback...")
        try:
            # Try with a simple document from existing documents or create minimal one
            if advanced_rag.documents:
                # Use first document as fallback base
                fallback_doc = [advanced_rag.documents[0]]
            else:
                fallback_doc = [Document(
                    page_content="This is a fallback document for testing the retrieval system when no other documents are available.", 
                    metadata={"source": "fallback", "type": "system"}
                )]
            
            advanced_rag.vectorstore = FAISS.from_documents(fallback_doc, advanced_rag.embed)
            print("‚úÖ Fallback vectorstore created successfully")
        except Exception as e:
            print(f"‚ùå Fallback vectorstore creation also failed: {e}")
            print("   System will operate with BM25-only search if available")
    
    index_time = time.time() - index_start
    print(f"üöÄ Advanced hybrid search system built in {index_time:.2f}s")
    
    # Save to cache
    try:
        if advanced_rag.vectorstore is not None:
            with open(vectorstore_cache, 'wb') as f:
                pickle.dump(advanced_rag.vectorstore, f)
            with open(docs_hash_cache, 'w') as f:
                f.write(docs_hash)
            print("Vectorstore cached for future use.")
        else:
            print("Warning: Cannot cache - vectorstore is None")
    except Exception as e:
        print(f"Warning: Could not cache vectorstore: {e}")

class AdvancedRetriever:
    def __init__(self, rag_system):
        self.rag_system = rag_system
    
    def invoke(self, query: str) -> List[Document]:
        """Advanced retrieval with hybrid search and reranking"""
        # Check if vectorstore is available
        if self.rag_system.vectorstore is None and not self.rag_system.bm25_retriever:
            print("‚ö†Ô∏è No retrieval systems available, returning document fallback")
            if self.rag_system.documents:
                return self.rag_system.documents[:3]  # Return first 3 docs as fallback
            else:
                return [Document(page_content="No documents available for search", metadata={"source": "system"})]
        
        # Analyze query structure
        query_analysis = self.rag_system.structured_query_analysis(query)
        
        # Perform hybrid search
        hybrid_results = self.rag_system.hybrid_search(query, k=8)
        
        # If no results, return fallback
        if not hybrid_results:
            print("‚ö†Ô∏è No search results found, using document fallback")
            if self.rag_system.documents:
                return self.rag_system.documents[:3]
            else:
                return [Document(page_content="No relevant documents found", metadata={"source": "system"})]
        
        # Rerank results using LLM
        reranked_results = self.rag_system.rerank_documents(query, hybrid_results)
        
        return reranked_results
    
    def get_relevant_documents(self, query: str) -> List[Document]:
        return self.invoke(query)

# Create advanced retriever
retriever = AdvancedRetriever(advanced_rag)

# Enhanced system status with advanced features
print(f"\nüéØ ADVANCED RAG SYSTEM STATUS:")
print(f"üìä System Components:")
print(f"   ‚Ä¢ Embedding Model: {'‚úÖ Active' if advanced_rag.embed else '‚ùå Failed'}")
print(f"   ‚Ä¢ Vector Store: {'‚úÖ Active' if advanced_rag.vectorstore else '‚ùå Failed'}")
print(f"   ‚Ä¢ BM25 Retriever: {'‚úÖ Active' if advanced_rag.bm25_retriever else '‚ùå Failed'}")

if advanced_rag.vectorstore:
    try:
        index_size = advanced_rag.vectorstore.index.ntotal if hasattr(advanced_rag.vectorstore, 'index') else 'N/A'
        print(f"üìà Vector Store Statistics:")
        print(f"   ‚Ä¢ Index size: {index_size} embeddings")
    except:
        print(f"üìà Vector Store: Active (size unknown)")

if 'all_split_docs' in locals() and all_split_docs:
    print(f"üìö Document Statistics:")
    print(f"   ‚Ä¢ Total documents: {len(advanced_rag.documents)}")
    print(f"   ‚Ä¢ Document chunks: {len(all_split_docs):,}")
    print(f"   ‚Ä¢ Average chunk size: {sum(len(doc.page_content) for doc in all_split_docs) // len(all_split_docs):,} characters")

# System readiness check
ready_components = []
if advanced_rag.embed: ready_components.append("Embeddings")
if advanced_rag.vectorstore: ready_components.append("Vector Search")
if advanced_rag.bm25_retriever: ready_components.append("BM25 Search")

if len(ready_components) >= 2:
    print(f"üöÄ HYBRID SEARCH READY! Active: {', '.join(ready_components)}")
elif len(ready_components) == 1:
    print(f"‚ö†Ô∏è  PARTIAL SYSTEM READY! Active: {', '.join(ready_components)} only")
else:
    print(f"‚ùå SYSTEM NOT READY! All components failed to initialize")
print(f"üîç Advanced Features:")
print(f"   ‚Ä¢ Multi-format support: PDF, Word (DOCX/DOC) documents")
print(f"   ‚Ä¢ Hybrid search: Vector + BM25 keyword matching")
print(f"   ‚Ä¢ Query rewriting: Multiple query variations")
print(f"   ‚Ä¢ LLM-based reranking: Relevance scoring")
print(f"   ‚Ä¢ Structured query analysis: Optimized retrieval strategy")
print(f"   ‚Ä¢ Offline operation: 100% local with Ollama models")
print(f"üöÄ Next-generation RAG system ready for high-accuracy retrieval!")